{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quora pairs Kaggle competition\n",
    "Third Quora Submission\n",
    "@author: Luis Duque\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re, math\n",
    "from string import punctuation\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import xor\n",
    "from IPython.display import clear_output\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "###   Tunning a Random Forest Classifier: select parameters and features \n",
    "def TuningParametersRandomForest(X_train, Y_train, sample_leaf_options, n_estimators_options, max_features_options):\n",
    "    best_score = 1\n",
    "    for leaf_size in sample_leaf_options:\n",
    "        for num_estimators in n_estimators_options:\n",
    "            for maxfeatures in max_features_options:\n",
    "                model = RandomForestClassifier(warm_start=True, oob_score = True, n_estimators = num_estimators, min_samples_leaf = leaf_size, max_features= maxfeatures, n_jobs = -1,random_state =50) \n",
    "                model.fit(X_train, Y_train)\n",
    "                oob_error = 1 - model.oob_score_ \n",
    "                \n",
    "                if  oob_error < best_score:\n",
    "                    best_score = oob_error\n",
    "                    best_model = model                \n",
    "                \n",
    "                best_score = min(oob_error, best_score)\n",
    "                print \"samples_leaf=\", leaf_size, \" estimators=\", num_estimators, \" error=\", oob_error, \" max_features=\", maxfeatures , \" best_score =\", best_score\n",
    "    \n",
    "    print \"So far our error is \", 1 - best_model.oob_score_ \n",
    "    return best_model\n",
    "    \n",
    "    \n",
    "###############################################################################################################\n",
    "#### Feature selection for a Random Forest Classifier \n",
    "def BackwardSubsetSelection(X, y):\n",
    "    ## Only for a piece of the training set (temporal)\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    best_model = RandomForestClassifier(warm_start=True, oob_score = True, n_jobs = -1,random_state =50)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    currenterror = 1 - best_model.oob_score_ \n",
    "    print \"Error we want to beat = \", currenterror\n",
    "    currentfeatures = list(X_train)## get the features from the data frame\n",
    "    removed_features = 0\n",
    "    while True:\n",
    "        newfeatures, newerror = RemoveOneFeature( X_train, y_train, currentfeatures, currenterror)\n",
    "        if(len(newfeatures)< len(currentfeatures)):\n",
    "            currenterror = newerror\n",
    "            currentfeatures = newfeatures\n",
    "            removed_features = removed_features +1\n",
    "        else:\n",
    "            break\n",
    "    print \"We removed the features: \", np.setdiff1d(list(X), currentfeatures) \n",
    "    print \"Our error is now\", newerror\n",
    "    return currentfeatures\n",
    "\n",
    " \n",
    "def RemoveOneFeature(dftrain, target, currentfeatures, error):    \n",
    "    for feature in currentfeatures:\n",
    "            newfeatures = list(currentfeatures)\n",
    "            newfeatures.remove(feature)\n",
    "                        \n",
    "            ## We now make a Classification with one feature less \n",
    "            newmodel = RandomForestClassifier(warm_start=True, oob_score = True, n_jobs = -1,random_state =50)\n",
    "            X = dftrain[newfeatures]\n",
    "            Y = target\n",
    "            newmodel.fit(X, Y)\n",
    "            newerror = 1 - newmodel.oob_score_ ## Find the Out of Box Error\n",
    "            print \"Current features \", len(currentfeatures), \", removing feature \", feature, \"would give error=\", newerror\n",
    "            if newerror < error:\n",
    "                print \"Removed:\", feature\n",
    "                return newfeatures, newerror\n",
    "\n",
    "    return currentfeatures, error\n",
    "    \n",
    "    \n",
    "    \n",
    "def Submission2(X_train, Y_train, X_test, filename):\n",
    "    \n",
    "    clf = RandomForestClassifier(n_jobs = -1,random_state =50) #Initialize with whatever parameters you want to\n",
    "    \n",
    "    # Parameters that will be used for tunning the random forest\n",
    "    param_grid = {\n",
    "                 'n_estimators': [1, 5, 20, 200],\n",
    "                 'min_samples_leaf' : [1, 5, 10, 20]\n",
    "    }\n",
    "    \n",
    "    Gmodel = GridSearchCV(clf, param_grid, cv=5)\n",
    "    Gmodel.fit(X_train, Y_train)    \n",
    "    \n",
    "    Y_test =Gmodel.predict(X_test)\n",
    "    temp = {'is_duplicate': Y_test}\n",
    "    submissionDf = pd.DataFrame.from_dict(temp)\n",
    "    submissionDf.to_csv(filename, header = True)\n",
    "    \n",
    "    best_model = Gmodel.best_estimator_ \n",
    "    \n",
    "    print \"\\n n_estimators = \", best_model.n_estimators, \" min_samples_leaf = \", best_model.min_samples_leaf,  \"max_features=\",  best_model.max_features\n",
    "    print \"\\n Confussion Matrix for the whole data set= \"\n",
    "    Y_predict = Gmodel.predict(X_train)\n",
    "    CM = confusion_matrix(list(Y_train), list(Y_predict))\n",
    "    print CM\n",
    "    \n",
    "    print \"\\n Classification error rate in training data set\", float(CM[0,1] + CM[1,0])/(CM[0,0] + CM[1,1]+ CM[0,1] + CM[1,0]) \n",
    "    \n",
    "    \n",
    "    return submissionDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "###############################             MAIN                 ################################################\n",
    "#################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Loading My features (Must run DictinaryCreation.ipynb and FeatureEngineering.ipynb before)\n",
    "Mtrain = pd.DataFrame.from_csv(\"Mtrain.csv\")\n",
    "Mtest = pd.DataFrame.from_csv(\"Mtest.csv\")\n",
    "\n",
    "## Load Abhisheks features\n",
    "Atrain = pd.DataFrame.from_csv(\"Abhitrain.csv\")\n",
    "Atest = pd.DataFrame.from_csv(\"Abhitest.csv\")\n",
    "\n",
    "## Loading the target\n",
    "Ytrain = pd.DataFrame.from_csv(\"./data/train.csv\")[\"is_duplicate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIRST EXPERIMENT: Only Abhisheks features\n",
    "Xtrain = Atrain\n",
    "Xtest = Atest\n",
    "\n",
    "## Deal with infinite rows and things like that\n",
    "Xtrain = Xtrain.replace([np.inf, -np.inf], np.nan)\n",
    "Xtrain = Xtrain.replace([np.nan], 0)\n",
    "Xtest =  Xtest.replace([np.inf, -np.inf], np.nan)\n",
    "Xtest  = Xtest.replace([np.nan], 0)\n",
    "\n",
    "## Feature selection\n",
    "#features = BackwardSubsetSelection(Xtrain, Ytrain)\n",
    "\n",
    "## Tuning parameters\n",
    "#best_model = TuningParametersRandomForest(Xtrain, Ytrain, sample_leaf_options, n_estimators_options, max_features_options)\n",
    "## Generate submission\n",
    "Submissiondf = Submission2(Xtrain, Ytrain, Xtest, \"SubmissionA.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gpu/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/Users/gpu/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error we want to beat =  0.21985632661137866\n",
      "Current features  78 , removing feature  len_q1 would give error= 0.22041109104850476\n",
      "Current features  78 , removing feature  len_q2 would give error= 0.22008247262396508\n",
      "Current features  78 , removing feature  diff_len would give error= 0.22078211185040442\n",
      "Current features  78 , removing feature  len_char_q1 would give error= 0.22159835761458357\n",
      "Current features  78 , removing feature  len_char_q2 would give error= 0.22151355285986363\n",
      "Current features  78 , removing feature  len_word_q1 would give error= 0.22101532492588416\n",
      "Current features  78 , removing feature  len_word_q2 would give error= 0.22058070055794465\n",
      "Current features  78 , removing feature  common_words would give error= 0.22327678505174853\n",
      "Current features  78 , removing feature  fuzz_qratio would give error= 0.21976092126231872\n",
      "Removed: fuzz_qratio\n",
      "Current features  77 , removing feature  len_q1 would give error= 0.22041462457995142\n",
      "Current features  77 , removing feature  len_q2 would give error= 0.2212379374070239\n",
      "Current features  77 , removing feature  diff_len would give error= 0.22044289283152474\n",
      "Current features  77 , removing feature  len_char_q1 would give error= 0.22022381388183165\n",
      "Current features  77 , removing feature  len_char_q2 would give error= 0.2191602209163861\n",
      "Removed: len_char_q2\n",
      "Current features  76 , removing feature  len_q1 would give error= 0.2185453864446667\n",
      "Removed: len_q1\n",
      "Current features  75 , removing feature  len_q2 would give error= 0.22025208213340497\n",
      "Current features  75 , removing feature  diff_len would give error= 0.22079977950763774\n",
      "Current features  75 , removing feature  len_char_q1 would give error= 0.22093405370261088\n",
      "Current features  75 , removing feature  len_word_q1 would give error= 0.22048176167743805\n",
      "Current features  75 , removing feature  len_word_q2 would give error= 0.22002593612081855\n",
      "Current features  75 , removing feature  common_words would give error= 0.22447465221216734\n",
      "Current features  75 , removing feature  fuzz_WRatio would give error= 0.22004360377805188\n",
      "Current features  75 , removing feature  fuzz_partial_ratio would give error= 0.22033335335667825\n",
      "Current features  75 , removing feature  fuzz_partial_token_set_ratio would give error= 0.2189199407780129\n",
      "Current features  75 , removing feature  fuzz_partial_token_sort_ratio would give error= 0.22005420437239176\n",
      "Current features  75 , removing feature  fuzz_token_set_ratio would give error= 0.22055949936926467\n",
      "Current features  75 , removing feature  fuzz_token_sort_ratio would give error= 0.2206831729698978\n",
      "Current features  75 , removing feature  wmd would give error= 0.22202944845107653\n",
      "Current features  75 , removing feature  norm_wmd would give error= 0.22018141150447168\n",
      "Current features  75 , removing feature  cosine_distance would give error= 0.21954184231262563\n",
      "Current features  75 , removing feature  cityblock_distance would give error= 0.2201390091271117\n",
      "Current features  75 , removing feature  jaccard_distance would give error= 0.22076091066172443\n",
      "Current features  75 , removing feature  canberra_distance would give error= 0.22065843824977116\n",
      "Current features  75 , removing feature  euclidean_distance would give error= 0.219852793079932\n",
      "Current features  75 , removing feature  minkowski_distance would give error= 0.22077151125606442\n",
      "Current features  75 , removing feature  braycurtis_distance would give error= 0.21929449511135923\n",
      "Current features  75 , removing feature  skew_q1vec would give error= 0.21957011056419895\n",
      "Current features  75 , removing feature  skew_q2vec would give error= 0.22105772730324413\n",
      "Current features  75 , removing feature  kur_q1vec would give error= 0.2190895502874528\n",
      "Current features  75 , removing feature  kur_q2vec would give error= 0.2196337141302389\n",
      "Current features  75 , removing feature  index would give error= 0.22008600615541174\n",
      "Current features  75 , removing feature  F0 would give error= 0.22007540556107175\n",
      "Current features  75 , removing feature  F1 would give error= 0.22018141150447168\n",
      "Current features  75 , removing feature  F2 would give error= 0.22002593612081855\n",
      "Current features  75 , removing feature  F3 would give error= 0.2200118019950319\n",
      "Current features  75 , removing feature  F4 would give error= 0.2210612608346908\n",
      "Current features  75 , removing feature  F5 would give error= 0.22111426380639077\n",
      "Current features  75 , removing feature  F6 would give error= 0.22061956940385796\n",
      "Current features  75 , removing feature  F7 would give error= 0.22072910887870445\n",
      "Current features  75 , removing feature  F8 would give error= 0.2197538541994255\n",
      "Current features  75 , removing feature  F9 would give error= 0.22028388391642495\n",
      "Current features  75 , removing feature  F10 would give error= 0.22013194206421838\n",
      "Current features  75 , removing feature  F11 would give error= 0.22005773790383842\n",
      "Current features  75 , removing feature  F12 would give error= 0.22055949936926467\n",
      "Current features  75 , removing feature  F13 would give error= 0.220566566432158\n",
      "Current features  75 , removing feature  F14 would give error= 0.2205948346837313\n",
      "Current features  75 , removing feature  F15 would give error= 0.2197255859478522\n",
      "Current features  75 , removing feature  F16 would give error= 0.2199517319604386\n",
      "Current features  75 , removing feature  F17 would give error= 0.21901534612707285\n",
      "Current features  75 , removing feature  F18 would give error= 0.21888813899499293\n",
      "Current features  75 , removing feature  F19 would give error= 0.22101885845733082\n",
      "Current features  75 , removing feature  F20 would give error= 0.21962664706734558\n",
      "Current features  75 , removing feature  F21 would give error= 0.2218916407246566\n",
      "Current features  75 , removing feature  F22 would give error= 0.21953124171828564\n",
      "Current features  75 , removing feature  F23 would give error= 0.21899414493839287\n",
      "Current features  75 , removing feature  F24 would give error= 0.22028741744787161\n",
      "Current features  75 , removing feature  F25 would give error= 0.22103652611456415\n",
      "Current features  75 , removing feature  F26 would give error= 0.2209305201711642\n",
      "Current features  75 , removing feature  F27 would give error= 0.2204676275516514\n",
      "Current features  75 , removing feature  F28 would give error= 0.22081038010197773\n",
      "Current features  75 , removing feature  F29 would give error= 0.2209623219541842\n",
      "Current features  75 , removing feature  F30 would give error= 0.219842192485592\n",
      "Current features  75 , removing feature  F31 would give error= 0.22031568569944493\n",
      "Current features  75 , removing feature  F32 would give error= 0.21948177227803234\n",
      "Current features  75 , removing feature  F33 would give error= 0.2208633830736777\n",
      "Current features  75 , removing feature  F34 would give error= 0.22148175107684376\n",
      "Current features  75 , removing feature  F35 would give error= 0.22005773790383842\n",
      "Current features  75 , removing feature  F36 would give error= 0.22063723706109117\n",
      "Current features  75 , removing feature  F37 would give error= 0.2228244930265757\n",
      "Current features  75 , removing feature  F38 would give error= 0.22067610590700448\n",
      "Current features  75 , removing feature  F39 would give error= 0.22054183171203134\n",
      "Current features  75 , removing feature  F40 would give error= 0.2189800108126062\n",
      "Current features  75 , removing feature  F41 would give error= 0.22155948876867027\n",
      "Current features  75 , removing feature  F42 would give error= 0.22053476464913802\n",
      "Current features  75 , removing feature  F43 would give error= 0.2280541195676371\n",
      "Current features  75 , removing feature  F44 would give error= 0.219474705215139\n",
      "Current features  75 , removing feature  F45 would give error= 0.220902251919591\n",
      "Current features  75 , removing feature  F46 would give error= 0.2191990897622993\n",
      "Current features  75 , removing feature  F47 would give error= 0.21929096157991257\n",
      "Current features  75 , removing feature  F48 would give error= 0.21957364409564561\n",
      "We removed the features:  ['fuzz_qratio' 'len_char_q2' 'len_q1']\n",
      "Our error is now 0.2185453864446667\n",
      "samples_leaf= 1  estimators= 20  error= 0.18834994682035178  max_features= None  best_score = 0.18834994682035178\n",
      "samples_leaf= 1  estimators= 50  error= 0.17273244453239012  max_features= None  best_score = 0.17273244453239012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples_leaf= 1  estimators= 100  error= 0.16650918894852706  max_features= None  best_score = 0.16650918894852706\n",
      "samples_leaf= 1  estimators= 200  error= 0.16275940537732814  max_features= None  best_score = 0.16275940537732814\n",
      "samples_leaf= 3  estimators= 20  error= 0.1835415172277326  max_features= None  best_score = 0.16275940537732814\n",
      "samples_leaf= 3  estimators= 50  error= 0.17079571594647402  max_features= None  best_score = 0.16275940537732814\n",
      "samples_leaf= 3  estimators= 100  error= 0.1660788048183235  max_features= None  best_score = 0.16275940537732814\n",
      "samples_leaf= 3  estimators= 200  error= 0.1637735289025205  max_features= None  best_score = 0.16275940537732814\n",
      "samples_leaf= 5  estimators= 20  error= 0.18193623389151348  max_features= None  best_score = 0.16275940537732814\n",
      "samples_leaf= 5  estimators= 50  error= 0.1712755695169309  max_features= None  best_score = 0.16275940537732814\n",
      "samples_leaf= 5  estimators= 100  error= 0.16752578594573198  max_features= None  best_score = 0.16275940537732814\n",
      "samples_leaf= 5  estimators= 200  error= 0.16488659130821937  max_features= None  best_score = 0.16275940537732814\n",
      "samples_leaf= 10  estimators= 20  error= 0.18008360335402807  max_features= None  best_score = 0.16275940537732814\n",
      "samples_leaf= 10  estimators= 50  error= 0.17160454129461522  max_features= None  best_score = 0.16275940537732814\n",
      "samples_leaf= 10  estimators= 100  error= 0.16899997526527988  max_features= None  best_score = 0.16275940537732814\n",
      "samples_leaf= 10  estimators= 200  error= 0.16754310024982066  max_features= None  best_score = 0.16275940537732814\n",
      "So far our error is  0.16275940537732814\n",
      "\n",
      " n_estimators =  200  min_samples_leaf =  1 max_features None\n",
      "\n",
      " Features =  ['len_q2', 'diff_len', 'len_char_q1', 'len_word_q1', 'len_word_q2', 'common_words', 'fuzz_WRatio', 'fuzz_partial_ratio', 'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio', 'fuzz_token_set_ratio', 'fuzz_token_sort_ratio', 'wmd', 'norm_wmd', 'cosine_distance', 'cityblock_distance', 'jaccard_distance', 'canberra_distance', 'euclidean_distance', 'minkowski_distance', 'braycurtis_distance', 'skew_q1vec', 'skew_q2vec', 'kur_q1vec', 'kur_q2vec', 'index', 'F0', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28', 'F29', 'F30', 'F31', 'F32', 'F33', 'F34', 'F35', 'F36', 'F37', 'F38', 'F39', 'F40', 'F41', 'F42', 'F43', 'F44', 'F45', 'F46', 'F47', 'F48']\n",
      "\\m FINAL ERROR =  0.16275940537732814\n",
      "\n",
      " Confussion Matrix = \n",
      "[[255027      0]\n",
      " [     0 149263]]\n"
     ]
    }
   ],
   "source": [
    "## SECOND EXPERIMENT: My features and Abhisheks variables together\n",
    "Atrain = Atrain.reset_index()\n",
    "Mtrain = Mtrain.reset_index()\n",
    "Atest = Atest.reset_index()\n",
    "Mtest = Mtest.reset_index()\n",
    "\n",
    "Xtrain = pd.concat([Atrain, Mtrain], axis=1)\n",
    "Xtest = pd.concat([Atest, Mtest], axis=1)\n",
    "\n",
    "## Deal with infinite rows and things like that\n",
    "Xtrain = Xtrain.replace([np.inf, -np.inf], np.nan)\n",
    "Xtrain = Xtrain.replace([np.nan], 0)\n",
    "Xtest =  Xtest.replace([np.inf, -np.inf], np.nan)\n",
    "Xtest  = Xtest.replace([np.nan], 0)\n",
    "\n",
    "## Feature selection\n",
    "#features = BackwardSubsetSelection(Xtrain, Ytrain)\n",
    "\n",
    "## Tuning parameters\n",
    "#best_model = TuningParametersRandomForest(Xtrain, Ytrain, sample_leaf_options, n_estimators_options, max_features_options)\n",
    "\n",
    "## Generate submission\n",
    "Submissiondf = Submission2(Xtrain, Ytrain, Xtest, \"SubmissionAM.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
