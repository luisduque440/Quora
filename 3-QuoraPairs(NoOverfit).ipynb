{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quora pairs Kaggle competition\n",
    "Third Quora Submission\n",
    "@author: Luis Duque\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re, math\n",
    "from string import punctuation\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import xor\n",
    "from IPython.display import clear_output\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "###   Tunning a Random Forest Classifier: select parameters and features \n",
    "def TuningParametersRandomForest(X_train, Y_train, sample_leaf_options, n_estimators_options, max_features_options):\n",
    "    best_score = 1\n",
    "    for leaf_size in sample_leaf_options:\n",
    "        for num_estimators in n_estimators_options:\n",
    "            for maxfeatures in max_features_options:\n",
    "                model = RandomForestClassifier(warm_start=True, oob_score = True, n_estimators = num_estimators, min_samples_leaf = leaf_size, max_features= maxfeatures, n_jobs = -1,random_state =50) \n",
    "                model.fit(X_train, Y_train)\n",
    "                oob_error = 1 - model.oob_score_ \n",
    "                \n",
    "                if  oob_error < best_score:\n",
    "                    best_score = oob_error\n",
    "                    best_model = model                \n",
    "                \n",
    "                best_score = min(oob_error, best_score)\n",
    "                print \"samples_leaf=\", leaf_size, \" estimators=\", num_estimators, \" error=\", oob_error, \" max_features=\", maxfeatures , \" best_score =\", best_score\n",
    "    \n",
    "    print \"So far our error is \", 1 - best_model.oob_score_ \n",
    "    return best_model\n",
    "    \n",
    "    \n",
    "###############################################################################################################\n",
    "#### Feature selection for a Random Forest Classifier \n",
    "def BackwardSubsetSelection(X, y):\n",
    "    ## Only for a piece of the training set (temporal)\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    best_model = RandomForestClassifier(warm_start=True, oob_score = True, n_jobs = -1,random_state =50)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    currenterror = 1 - best_model.oob_score_ \n",
    "    print \"Error we want to beat = \", currenterror\n",
    "    currentfeatures = list(X_train)## get the features from the data frame\n",
    "    removed_features = 0\n",
    "    while True:\n",
    "        newfeatures, newerror = RemoveOneFeature( X_train, y_train, currentfeatures, currenterror)\n",
    "        if(len(newfeatures)< len(currentfeatures)):\n",
    "            currenterror = newerror\n",
    "            currentfeatures = newfeatures\n",
    "            removed_features = removed_features +1\n",
    "        else:\n",
    "            break\n",
    "    print \"We removed the features: \", np.setdiff1d(list(X), currentfeatures) \n",
    "    print \"Our error is now\", newerror\n",
    "    return currentfeatures\n",
    "\n",
    " \n",
    "def RemoveOneFeature(dftrain, target, currentfeatures, error):    \n",
    "    for feature in currentfeatures:\n",
    "            newfeatures = list(currentfeatures)\n",
    "            newfeatures.remove(feature)\n",
    "                        \n",
    "            ## We now make a Classification with one feature less \n",
    "            newmodel = RandomForestClassifier(warm_start=True, oob_score = True, n_jobs = -1,random_state =50)\n",
    "            X = dftrain[newfeatures]\n",
    "            Y = target\n",
    "            newmodel.fit(X, Y)\n",
    "            newerror = 1 - newmodel.oob_score_ ## Find the Out of Box Error\n",
    "            print \"Current features \", len(currentfeatures), \", removing feature \", feature, \"would give error=\", newerror\n",
    "            if newerror < error:\n",
    "                print \"Removed:\", feature\n",
    "                return newfeatures, newerror\n",
    "\n",
    "    return currentfeatures, error\n",
    "    \n",
    "    \n",
    "    \n",
    "def Submission2(X, Y, X_test, filename):\n",
    "    \n",
    "    X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_jobs = -1,random_state =50) #Initialize with whatever parameters you want to\n",
    "    \n",
    "    # Parameters that will be used for tunning the random forest\n",
    "    param_grid = {\n",
    "                 'n_estimators': [150, 200, 300, 400,500],\n",
    "                 'min_samples_leaf' : [3 ,5, 10, 15]\n",
    "    }\n",
    "    \n",
    "    Gmodel = GridSearchCV(clf, param_grid, cv=5)\n",
    "    Gmodel.fit(X_train, Y_train)    \n",
    "    \n",
    "    Y_test =Gmodel.predict(X_test)\n",
    "    temp = {'is_duplicate': Y_test}\n",
    "    submissionDf = pd.DataFrame.from_dict(temp)\n",
    "    submissionDf.to_csv(filename, header = True)\n",
    "    \n",
    "    best_model = Gmodel.best_estimator_ \n",
    "    \n",
    "    print \"\\n Best model found has n_estimators = \", best_model.n_estimators, \" min_samples_leaf = \", best_model.min_samples_leaf,  \"max_features=\",  best_model.max_features\n",
    "    print \"\\n Confussion Matrix for the validation set= \"\n",
    "    Y_predict = Gmodel.predict(X_validation)\n",
    "    CM = confusion_matrix(list(Y_validation), list(Y_predict))\n",
    "    print CM\n",
    "    \n",
    "    print \"\\n Classification error rate in the validation data set\", float(CM[0,1] + CM[1,0])/(CM[0,0] + CM[1,1]+ CM[0,1] + CM[1,0]) \n",
    "    \n",
    "    \n",
    "    return submissionDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "###############################             MAIN                 ################################################\n",
    "#################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Loading My features (Must run DictinaryCreation.ipynb and FeatureEngineering.ipynb before)\n",
    "Mtrain = pd.DataFrame.from_csv(\"Mtrain.csv\")\n",
    "Mtest = pd.DataFrame.from_csv(\"Mtest.csv\")\n",
    "\n",
    "## Load Abhisheks features\n",
    "Atrain = pd.DataFrame.from_csv(\"Abhitrain.csv\")\n",
    "Atest = pd.DataFrame.from_csv(\"Abhitest.csv\")\n",
    "\n",
    "## Loading the target\n",
    "Ytrain = pd.DataFrame.from_csv(\"./data/train.csv\")[\"is_duplicate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best model found has n_estimators =  500  min_samples_leaf =  3 max_features= auto\n",
      "\n",
      " Confussion Matrix for the validation set= \n",
      "[[43797  7282]\n",
      " [ 6179 23600]]\n",
      "\n",
      " Classification error rate in the validation data set 0.166477033812\n"
     ]
    }
   ],
   "source": [
    "## SECOND EXPERIMENT: My features and Abhisheks variables together\n",
    "Atrain = Atrain.reset_index()\n",
    "Mtrain = Mtrain.reset_index()\n",
    "Atest = Atest.reset_index()\n",
    "Mtest = Mtest.reset_index()\n",
    "\n",
    "Xtrain = pd.concat([Atrain, Mtrain], axis=1)\n",
    "Xtest = pd.concat([Atest, Mtest], axis=1)\n",
    "\n",
    "## Deal with infinite rows and things like that\n",
    "Xtrain = Xtrain.replace([np.inf, -np.inf], np.nan)\n",
    "Xtrain = Xtrain.replace([np.nan], 0)\n",
    "Xtest =  Xtest.replace([np.inf, -np.inf], np.nan)\n",
    "Xtest  = Xtest.replace([np.nan], 0)\n",
    "\n",
    "## Feature selection\n",
    "#features = BackwardSubsetSelection(Xtrain, Ytrain)\n",
    "\n",
    "## Tuning parameters\n",
    "#best_model = TuningParametersRandomForest(Xtrain, Ytrain, sample_leaf_options, n_estimators_options, max_features_options)\n",
    "\n",
    "## Generate submission\n",
    "Submissiondf = Submission2(Xtrain, Ytrain, Xtest, \"SubmissionAM.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best model found has n_estimators =  500  min_samples_leaf =  3 max_features= auto\n",
      "\n",
      " Confussion Matrix for the validation set= \n",
      "[[40049 11030]\n",
      " [ 7845 21934]]\n",
      "\n",
      " Classification error rate in the validation data set 0.233433921195\n"
     ]
    }
   ],
   "source": [
    "## FIRST EXPERIMENT: Only Abhisheks features\n",
    "Xtrain = Atrain\n",
    "Xtest = Atest\n",
    "\n",
    "## Deal with infinite rows and things like that\n",
    "Xtrain = Xtrain.replace([np.inf, -np.inf], np.nan)\n",
    "Xtrain = Xtrain.replace([np.nan], 0)\n",
    "Xtest =  Xtest.replace([np.inf, -np.inf], np.nan)\n",
    "Xtest  = Xtest.replace([np.nan], 0)\n",
    "\n",
    "## Feature selection\n",
    "#features = BackwardSubsetSelection(Xtrain, Ytrain)\n",
    "\n",
    "## Tuning parameters\n",
    "#best_model = TuningParametersRandomForest(Xtrain, Ytrain, sample_leaf_options, n_estimators_options, max_features_options)\n",
    "## Generate submission\n",
    "Submissiondf = Submission2(Xtrain, Ytrain, Xtest, \"SubmissionA.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
