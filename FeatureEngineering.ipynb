{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quora pairs Kaggle competition\n",
    "Feature Engineering\n",
    "@author: Luis Duque\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import re, math\n",
    "from string import punctuation\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import xor\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gpu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "## Word2Vect and NLTK models loading\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from gensim.models import Word2Vec\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadDictionary(): # Loads Dictionary.csv \n",
    "    Dictionary ={}\n",
    "    DictionaryDf = pd.DataFrame.from_csv(\"Dictionary.csv\")\n",
    "    Words = DictionaryDf['Words']\n",
    "    Quotient1 = DictionaryDf[ 'Quotient1']\n",
    "    Quotient2 = DictionaryDf[ 'Quotient2']\n",
    "    for i in range(0, len(Words)):   \n",
    "        Dictionary[Words[i]] = [Quotient1[i], Quotient2[i]]\n",
    "    return Dictionary    \n",
    "Dictionary = LoadDictionary();                ## Loading Dictionary.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "###################### FEATURE ENGINEERING ##############################################################################\n",
    "#########################################################################################################################\n",
    "WORD = re.compile(r'\\w+') ## used when computing the cosine distance\n",
    "StopWords = {\"ourselves\", \"hers\", \"between\", \"yourself\", \"but\", \"again\", \"there\", \"about\", \"once\", \"during\", \"out\", \"very\", \"having\", \"with\", \"they\", \"own\", \"an\", \"be\", \"some\", \"for\", \"do\", \"its\", \"yours\", \"such\", \"into\", \"of\", \"most\", \"itself\", \"other\", \"off\", \"is\", \"s\", \"am\", \"or\", \"who\", \"as\", \"from\", \"him\", \"each\", \"the\", \"themselves\", \"until\", \"below\", \"are\", \"we\", \"these\", \"your\", \"his\", \"through\", \"don\", \"nor\", \"me\", \"were\", \"her\", \"more\", \"himself\", \"this\", \"down\", \"should\", \"our\", \"their\", \"while\", \"above\", \"both\", \"up\", \"to\", \"ours\", \"had\", \"she\", \"all\", \"no\", \"when\", \"at\", \"any\", \"before\", \"them\", \"same\", \"and\", \"been\", \"have\", \"in\", \"will\", \"on\", \"does\", \"yourselves\", \"then\", \"that\", \"because\", \"what\", \"over\", \"why\", \"so\", \"can\", \"did\", \"not\", \"now\", \"under\", \"he\", \"you\", \"herself\", \"has\", \"just\", \"where\", \"too\", \"only\", \"myself\", \"which\", \"those\", \"i\", \"after\", \"few\", \"whom\", \"t\", \"being\", \"if\", \"theirs\", \"my\", \"against\", \"a\", \"by\", \"doing\", \"it\", \"how\", \"further\", \"was\", \"here\", \"than\"} \n",
    "PiecesOfSpeech = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\"]\n",
    "PartsOfSpeech = PiecesOfSpeech + [\"ADJETIVES\", \"NOUNS\", \"PROPERNOUNS\", \"PRONOUNS\", \"ADVERBS\", \"VERBS\", \"WH\"]\n",
    "RelevantPartsOfSpeech =  [\"ADJETIVES\", \"NOUNS\", \"PROPERNOUNS\", \"PRONOUNS\", \"ADVERBS\", \"VERBS\", \"WH\"]\n",
    "#StartWords = [\"what\", \"how\", \"why\", \"is\", \"which\", \"can\", \"i\", \"who\", \"do\", \"where\", \"if\", \"does\", \"are\", \"should\", \"when\", \"will\", \"did\", \"would\", \"has\", \"have\", \"was\", \"could\", \"for\", \"after\", \"am\", \"at\", \"were\", \"from\" , \"with\"]\n",
    "StartWords = [\"what\", \"how\", \"why\", \"is\", \"which\", \"can\", \"i\", \"who\", \"do\", \"where\"]\n",
    "\n",
    "#####\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def FuzzySame(Word1, Word2):   ## THIS SHOULD BE IMPROVED\n",
    "    ## we compare them in a more straightforward way\n",
    "    comparison = similar(Word1, Word2)\n",
    "    #print \"comparing \", Word1, \" and \", Word2, \" = \\t\", comparison, \"(usign similar)\"\n",
    "\n",
    "    if comparison > 0.8:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "def FuzzyWordInList(Word, List):\n",
    "    if Word in List: \n",
    "        return 1\n",
    "    \n",
    "    for cWord in List:\n",
    "        if FuzzySame(Word, cWord):\n",
    "            return 1;\n",
    "    return 0\n",
    "\n",
    "\n",
    "## This function is not really fuzzy but if it was the generation of the dictionary would be too slow.\n",
    "def FuzzyFindRepresentative(Word, List): ## will also work for if List is a dictionary, \n",
    "    if Word in List: \n",
    "        return Word\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "    \n",
    "def FuzzyIntersection(Bag1, Bag2):\n",
    "    Intersection = []\n",
    "    for Word in Bag1:\n",
    "        if FuzzyWordInList(Word, Bag2):\n",
    "            Intersection.append(Word)\n",
    "    return Intersection\n",
    "\n",
    "\n",
    "def FuzzyUnion(Bag1, Bag2):\n",
    "    Set1 = set(Bag1)-set(FuzzyIntersection(Bag1, Bag2))\n",
    "    Set2 = set(Bag2)\n",
    "    return list(Set1.union(Set2))\n",
    "\n",
    "\n",
    "def FuzzyWordsInOneList(Bag1, Bag2):\n",
    "    Lista = []\n",
    "    for Word in Bag1:\n",
    "        if not FuzzyWordInList(Word, Bag2):\n",
    "            Lista.append(Word)\n",
    "        \n",
    "    for Word in Bag2:\n",
    "        if not FuzzyWordInList(Word, Bag1):\n",
    "            Lista.append(Word)\n",
    "    return Lista\n",
    "\n",
    "\n",
    "#########################################################################################################################\n",
    "### Very basic features\n",
    "def strip_punctuation(s):\n",
    "    return ''.join(c for c in s if c not in punctuation+\"’\")\n",
    "\n",
    "def strip_punctuation_nums(s):\n",
    "    s = s.lower()\n",
    "    return ''.join(c for c in s if c not in punctuation+\"’0123456789\")\n",
    "\n",
    "## Auxiliar function that converts a string of characters into a Bag of words\n",
    "def StringToList_Nonums(Doc):\n",
    "    nopunctuationlower = strip_punctuation_nums(Doc)\n",
    "    tokens = nopunctuationlower.split()\n",
    "    return tokens\n",
    "\n",
    "def num_words(document):\n",
    "    tokens = set(document.split())\n",
    "    return len(tokens)\n",
    "\n",
    "def num_stopwords(document):\n",
    "    filtered_tokens = []\n",
    "    nopunctuation = strip_punctuation(document)\n",
    "    tokens = set(nopunctuation.lower().split())\n",
    "    filtered_sentence = [w for w in tokens if not w in StopWords]\n",
    "    return len(filtered_sentence)\n",
    "\n",
    "def num_capitals(document):\n",
    "    capitals = \"\".join([c for c in document if c.isupper()])\n",
    "    return len(capitals)\n",
    "\n",
    "def num_questionmarks(document):\n",
    "    questions = \"\".join([c for c in document if c==\"?\"])\n",
    "    return len(questions)\n",
    "\n",
    "def num_exclamationmarks(document):\n",
    "    exclamations = \"\".join([c for c in document if c==\"!\"])\n",
    "    return len(exclamations)\n",
    "\n",
    "def num_quotations(document):\n",
    "    quotations = \"\".join([c for c in document if c=='\"'  or c ==\"'\" or c ==\"‘\" or c ==\"’\" or c ==\"“\" or c ==\"”\" ])\n",
    "    return len(quotations)\n",
    "\n",
    "def num_dots(document):\n",
    "    dots = \"\".join([c for c in document if c=='.' ])\n",
    "    return len(dots)\n",
    "\n",
    "def num_commas(document):\n",
    "    commas = \"\".join([c for c in document if c==',' ])\n",
    "    return len(commas)\n",
    "\n",
    "def first_word(document):\n",
    "    nopunctuation = strip_punctuation(document)\n",
    "    tokens = list(nopunctuation.lower().split())\n",
    "    if len(tokens)== 0:\n",
    "        return \"\"\n",
    "    return tokens[0]\n",
    "\n",
    "\n",
    "def first_word_features(Doc):\n",
    "    word = first_word(Doc)\n",
    "    F = [0]*len(StartWords)\n",
    "    \n",
    "    if word in StartWords:\n",
    "        location = StartWords.index(word)\n",
    "        F[location]= 1\n",
    "    return F\n",
    "\n",
    "\n",
    "def GetPartsOfSpeech(Doc):\n",
    "    Parts = {} \n",
    "    for s in PartsOfSpeech:\n",
    "        Parts[s] = []\n",
    "    \n",
    "    tagged1 = pos_tag(Doc.split())\n",
    "    \n",
    "    for word,pos in tagged1:\n",
    "        if pos in PiecesOfSpeech:\n",
    "            Parts[pos].append(  strip_punctuation( word.lower()) )\n",
    "    Parts[\"ADJETIVES\"] =  Parts[\"JJ\"] + Parts[\"JJR\"] + Parts[\"JJS\"]\n",
    "    Parts[\"NOUNS\"] =  Parts[\"NN\"] + Parts[\"NNS\"]\n",
    "    Parts[\"PROPERNOUNS\"] = Parts[\"NNP\"] + Parts[\"NNPS\"] \n",
    "    Parts[\"PRONOUNS\"] = Parts[\"PRP\"] + Parts[\"PRP$\"]\n",
    "    Parts[\"ADVERBS\"] = Parts[\"RB\"] + Parts[\"RBR\"] + Parts[\"RBS\"]\n",
    "    Parts[\"VERBS\"] = Parts[\"VB\"] + Parts[\"VBD\"] + Parts[\"VBG\"] + Parts[\"VBN\"] + Parts[\"VBP\"] + Parts[\"VBZ\"]\n",
    "    Parts[\"WH\"] =  Parts[\"WDT\"] + Parts[\"WP\"] + Parts[\"WP$\"] + Parts[\"WRB\"]       \n",
    "    return Parts\n",
    "\n",
    "#########################################################################################################################\n",
    "### Definition of the distances relevant to us: JaccardList1List2, CosineList1List2, WordMoversList1List2\n",
    "def JaccardList1List2(List1, List2):   \n",
    "    List1.append(\"a\")  # avoids divisions by zero\n",
    "    List2.append(\"a\")\n",
    "    Set1 = set(List1)\n",
    "    Set2 = set(List2)\n",
    "    intersection = Set1.intersection(Set2)\n",
    "    union = Set1.union(Set2)\n",
    "    return float(len(intersection))/(len(union))    \n",
    "\n",
    "def WordMoversList1List2(List1, List2):\n",
    "    List1.append(\"the\")\n",
    "    List2.append(\"the\")\n",
    "    Set1 = set(List1)\n",
    "    Set2 = set(List2)\n",
    "    Out = model.wmdistance(Set1, Set2)\n",
    "    if Out == float(\"Inf\"):\n",
    "        print \"WARNING!\"\n",
    "    return Out\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    text = text + \" a\"\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "def CosineList1List2(List1, List2):\n",
    "    text1 = \"\"\n",
    "    text2 = \"\"\n",
    "    for w in List1:\n",
    "        text1 = text1 + w + \" \"\n",
    "    for w in List2:\n",
    "        text2 = text2 + w+ \" \" \n",
    "    return get_cosine(text_to_vector(text1), text_to_vector(text2))\n",
    "\n",
    "#########################################################################################################################\n",
    "### Functions that create many features provided that we have a defined distance between words\n",
    "def DistanceStr1Str2(Str1, Str2, DistanceSet1Set2):\n",
    "    nopunctuation1 = strip_punctuation(Str1)\n",
    "    nopunctuation2 = strip_punctuation(Str2)\n",
    "    tokens1 = set(nopunctuation1.lower().split())\n",
    "    tokens2 = set(nopunctuation2.lower().split())\n",
    "    return DistanceSet1Set2(tokens1, tokens2)\n",
    "    \n",
    "def DistanceFeatures(Doc1, Doc2, DistanceList1List2):\n",
    "    PartsofDoc1 = GetPartsOfSpeech(Doc1)\n",
    "    PartsofDoc2 = GetPartsOfSpeech(Doc2)\n",
    "    Bag1 = strip_punctuation( Doc1.lower()).split()\n",
    "    Bag2 = strip_punctuation( Doc2.lower()).split()\n",
    "    Bag1NonStop = [w for w in Bag1 if w not in StopWords]\n",
    "    Bag2NonStop = [w for w in Bag2 if w not in StopWords]\n",
    "    \n",
    "    #We append the Distance  between the two documents and the distance without include stopwords\n",
    "    Distance = [DistanceList1List2(Bag1, Bag2), DistanceList1List2(Bag1NonStop, Bag2NonStop)] \n",
    "    \n",
    "    #We append the Distance between verbs, adverbs, nouns, ... (Only the most relevant ones: verbs, adverbs, ...)\n",
    "    for part in RelevantPartsOfSpeech:\n",
    "        Distance.append( DistanceList1List2( PartsofDoc1[part], PartsofDoc2[part])  )\n",
    "    return Distance\n",
    "\n",
    "def JaccardFeatures(Doc1, Doc2):\n",
    "    return DistanceFeatures(Doc1, Doc2, JaccardList1List2)\n",
    "\n",
    "def WordMoversFeatures(Doc1, Doc2):\n",
    "    return DistanceFeatures(Doc1, Doc2, WordMoversList1List2)\n",
    "\n",
    "def CosineFeatures(Doc1, Doc2):\n",
    "    return DistanceFeatures(Doc1, Doc2, CosineList1List2)\n",
    "\n",
    "\n",
    "#########################################################################################################################\n",
    "#### Features involving the words that are in exactly one questions or in both\n",
    "def InOneBagFeatures(Doc1, Doc2, lenmaxintersection, Dictionary):  \n",
    "    Bag1 = StringToList_Nonums(Doc1)\n",
    "    Bag2 = StringToList_Nonums(Doc2)\n",
    "    InOneBag = FuzzyWordsInOneList(Bag1, Bag2)        ## words in both bags\n",
    "    \n",
    "    OneBagQuotients = []\n",
    "    for cWord in InOneBag:\n",
    "        if cWord in Dictionary:\n",
    "            cQuotient = Dictionary[cWord]\n",
    "            OneBagQuotients.append(cQuotient[0])          ## this is Quotient1.  \n",
    "        \n",
    "    OneBagQuotients.sort(reverse=True)           \n",
    "    len1 = len(OneBagQuotients)\n",
    "    \n",
    "    while len(OneBagQuotients)< lenmaxintersection:   ## We truncate OneBagQuotients to have exactly lenmaxintersection elements \n",
    "        OneBagQuotients.append(0)\n",
    "\n",
    "    while len(OneBagQuotients)> lenmaxintersection:\n",
    "        del OneBagQuotients[-1]\n",
    "    \n",
    "    OneBagQuotients.append(len1)                      ## We append the size of the intersection\n",
    "    \n",
    "    return OneBagQuotients\n",
    "\n",
    "def InBothBagsFeatures(Doc1, Doc2, maxelements, Dictionary):       \n",
    "    Bag1 = StringToList_Nonums(Doc1)\n",
    "    Bag2 = StringToList_Nonums(Doc2)\n",
    "    InBothBags = FuzzyIntersection(Bag1, Bag2)         ## words in both bags\n",
    "    \n",
    "    BothBagsQuotients =[]\n",
    "\n",
    "    for cWord in InBothBags:\n",
    "        if cWord in Dictionary:\n",
    "            cQuotient = Dictionary[cWord]\n",
    "            BothBagsQuotients.append(cQuotient[1])  ## this is Quotient2.\n",
    "\n",
    "    BothBagsQuotients.sort(reverse=True)   ## Sort the coefficients.\n",
    "\n",
    "    len2 = len(BothBagsQuotients)           \n",
    "\n",
    "    while len(BothBagsQuotients)< maxelements:\n",
    "        BothBagsQuotients.append(0)\n",
    "\n",
    "    while len(BothBagsQuotients)> maxelements:\n",
    "        del BothBagsQuotients[-1]\n",
    "        \n",
    "    BothBagsQuotients.append(len2)\n",
    "    return BothBagsQuotients\n",
    "\n",
    "\n",
    "#########################################################################################################################\n",
    "### Generation of all the features of a pair of questions (Doc1, Doc2)\n",
    "def AllFeatures(Doc1, Doc2):\n",
    "    Feats = [len(Doc1), len(Doc2)]                                             #Length of question (2)  \n",
    "    #Feats = Feats + [num_words(Doc1), num_words(Doc2)]                        #Number of words (2)\n",
    "    #Feats = Feats + [num_words(Doc1)-num_stopwords(Doc1), num_words(Doc2)-num_stopwords(Doc2)] #Number of words except for stopwords (2) \n",
    "    Feats = Feats + first_word_features(Doc1) + first_word_features(Doc2)     #Indicators for questions starting with “Are”, “Can”, “How” etc… (10)  \n",
    "    #Feats = Feats + [num_exclamationmarks(Doc1), num_exclamationmarks(Doc2)]  #Number of exclamation marks (2)\n",
    "    Feats = Feats + [num_quotations(Doc1), num_quotations(Doc2)]              #Number of quotes (2) \n",
    "    Feats = Feats + [num_questionmarks(Doc1), num_questionmarks(Doc2)]        #Number of question marks (2) \n",
    "    Feats = Feats + [num_capitals(Doc1), num_capitals(Doc2)]                  #Number of capital letters (2) \n",
    "    #Feats = Feats + [num_dots(Doc1), num_dots(Doc2)]                          #Number of dots (2)  \n",
    "    #Feats = Feats + [num_commas(Doc1), num_commas(Doc2)]                      #Number of commas (2)  \n",
    "    #Feats = Feats + JaccardFeatures(Doc1, Doc2)                               #Jaccard Features (many)\n",
    "    Feats = Feats + WordMoversFeatures(Doc1, Doc2)                            #Word Mover’s Features(many)\n",
    "    #Feats = Feats + CosineFeatures(Doc1, Doc2)                                #Cosine Features(many)\n",
    "    Feats = Feats + InBothBagsFeatures(Doc1, Doc2, 5, Dictionary)              #Features involving words that are in both bags\n",
    "    Feats = Feats + InOneBagFeatures(Doc1, Doc2, 5, Dictionary)                #Features involving words that are only in one of the bags\n",
    "    return Feats\n",
    "\n",
    "#########################################################################################################################\n",
    "### Creates the data frame with the Features we will use in the classification##################################\n",
    "def FeatureEngineering(df, filename):\n",
    "    Features = []     \n",
    "    for index, row in df.iterrows():\n",
    "        Doc1 = str(row['question1'])   \n",
    "        Doc2 = str(row['question2']) \n",
    "        Features.append(AllFeatures(Doc1, Doc2))\n",
    "        if (index%10000) ==0:\n",
    "            print \"Current Row=\", index\n",
    "    labels = []\n",
    "    for i in range(0, len(Features[0])):\n",
    "        labels.append(\"F\"+str(i))\n",
    "    dfFeatures = pd.DataFrame.from_records(Features, columns=labels) \n",
    "    dfFeatures.to_csv(filename, header = True) \n",
    "    return dfFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "###############################             MAIN                 ################################################\n",
    "#################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Row= 0\n",
      "Current Row= 10000\n",
      "Current Row= 20000\n",
      "Current Row= 30000\n",
      "Current Row= 40000\n",
      "Current Row= 50000\n",
      "Current Row= 60000\n",
      "Current Row= 70000\n",
      "Current Row= 80000\n",
      "Current Row= 90000\n",
      "Current Row= 100000\n",
      "Current Row= 110000\n",
      "Current Row= 120000\n",
      "Current Row= 130000\n",
      "Current Row= 140000\n",
      "Current Row= 150000\n",
      "Current Row= 160000\n",
      "Current Row= 170000\n",
      "Current Row= 180000\n",
      "Current Row= 190000\n",
      "Current Row= 200000\n",
      "Current Row= 210000\n",
      "Current Row= 220000\n",
      "Current Row= 230000\n",
      "Current Row= 240000\n",
      "Current Row= 250000\n",
      "Current Row= 260000\n",
      "Current Row= 270000\n",
      "Current Row= 280000\n",
      "Current Row= 290000\n",
      "Current Row= 300000\n",
      "Current Row= 310000\n",
      "Current Row= 320000\n",
      "Current Row= 330000\n",
      "Current Row= 340000\n",
      "Current Row= 350000\n",
      "Current Row= 360000\n",
      "Current Row= 370000\n",
      "Current Row= 380000\n",
      "Current Row= 390000\n",
      "Current Row= 400000\n"
     ]
    }
   ],
   "source": [
    "dftrain = pd.DataFrame.from_csv(\"./data/train.csv\")  ## Loading data \n",
    "trainfeaturesdf = FeatureEngineering(dftrain, \"Mtrain.csv\") ## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    qid1  qid2                                          question1  \\\n",
      "id                                                                  \n",
      "0      1     2  What is the step by step guide to invest in sh...   \n",
      "1      3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
      "2      5     6  How can I increase the speed of my internet co...   \n",
      "3      7     8  Why am I mentally very lonely? How can I solve...   \n",
      "4      9    10  Which one dissolve in water quikly sugar, salt...   \n",
      "\n",
      "                                            question2  is_duplicate  \n",
      "id                                                                   \n",
      "0   What is the step by step guide to invest in sh...             0  \n",
      "1   What would happen if the Indian government sto...             0  \n",
      "2   How can Internet speed be increased by hacking...             0  \n",
      "3   Find the remainder when [math]23^{24}[/math] i...             0  \n",
      "4             Which fish would survive in salt water?             0  \n"
     ]
    }
   ],
   "source": [
    "print trainfeaturesdf.head()\n",
    "dftrain = {}\n",
    "trainfeaturesdf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Row= 0\n",
      "Current Row= 10000\n",
      "Current Row= 20000\n",
      "Current Row= 30000\n",
      "Current Row= 40000\n",
      "Current Row= 50000\n",
      "Current Row= 60000\n",
      "Current Row= 70000\n",
      "Current Row= 80000\n",
      "Current Row= 90000\n",
      "Current Row= 100000\n",
      "Current Row= 110000\n",
      "Current Row= 120000\n",
      "Current Row= 130000\n",
      "Current Row= 140000\n",
      "Current Row= 150000\n",
      "Current Row= 160000\n",
      "Current Row= 170000\n",
      "Current Row= 180000\n",
      "Current Row= 190000\n",
      "Current Row= 200000\n",
      "Current Row= 210000\n",
      "Current Row= 220000\n",
      "Current Row= 230000\n",
      "Current Row= 240000\n",
      "Current Row= 250000\n",
      "Current Row= 260000\n",
      "Current Row= 270000\n",
      "Current Row= 280000\n",
      "Current Row= 290000\n",
      "Current Row= 300000\n",
      "Current Row= 310000\n",
      "Current Row= 320000\n",
      "Current Row= 330000\n",
      "Current Row= 340000\n",
      "Current Row= 350000\n",
      "Current Row= 360000\n",
      "Current Row= 370000\n",
      "Current Row= 380000\n",
      "Current Row= 390000\n",
      "Current Row= 400000\n",
      "Current Row= 410000\n",
      "Current Row= 420000\n",
      "Current Row= 430000\n",
      "Current Row= 440000\n",
      "Current Row= 450000\n",
      "Current Row= 460000\n",
      "Current Row= 470000\n",
      "Current Row= 480000\n",
      "Current Row= 490000\n",
      "Current Row= 500000\n",
      "Current Row= 510000\n",
      "Current Row= 520000\n",
      "Current Row= 530000\n",
      "Current Row= 540000\n",
      "Current Row= 550000\n",
      "Current Row= 560000\n",
      "Current Row= 570000\n",
      "Current Row= 580000\n",
      "Current Row= 590000\n",
      "Current Row= 600000\n",
      "Current Row= 610000\n",
      "Current Row= 620000\n",
      "Current Row= 630000\n",
      "Current Row= 640000\n",
      "Current Row= 650000\n",
      "Current Row= 660000\n",
      "Current Row= 670000\n",
      "Current Row= 680000\n",
      "Current Row= 690000\n",
      "Current Row= 700000\n",
      "Current Row= 710000\n",
      "Current Row= 720000\n",
      "Current Row= 730000\n",
      "Current Row= 740000\n",
      "Current Row= 750000\n",
      "Current Row= 760000\n",
      "Current Row= 770000\n",
      "Current Row= 780000\n",
      "Current Row= 790000\n",
      "Current Row= 800000\n",
      "Current Row= 810000\n",
      "Current Row= 820000\n",
      "Current Row= 830000\n",
      "Current Row= 840000\n",
      "Current Row= 850000\n",
      "Current Row= 860000\n",
      "Current Row= 870000\n",
      "Current Row= 880000\n",
      "Current Row= 890000\n",
      "Current Row= 900000\n",
      "Current Row= 910000\n",
      "Current Row= 920000\n",
      "Current Row= 930000\n",
      "Current Row= 940000\n",
      "Current Row= 950000\n",
      "Current Row= 960000\n",
      "Current Row= 970000\n",
      "Current Row= 980000\n",
      "Current Row= 990000\n",
      "Current Row= 1000000\n",
      "Current Row= 1010000\n",
      "Current Row= 1020000\n",
      "Current Row= 1030000\n",
      "Current Row= 1040000\n",
      "Current Row= 1050000\n",
      "Current Row= 1060000\n",
      "Current Row= 1070000\n",
      "Current Row= 1080000\n",
      "Current Row= 1090000\n",
      "Current Row= 1100000\n",
      "Current Row= 1110000\n",
      "Current Row= 1120000\n",
      "Current Row= 1130000\n",
      "Current Row= 1140000\n",
      "Current Row= 1150000\n",
      "Current Row= 1160000\n",
      "Current Row= 1170000\n",
      "Current Row= 1180000\n",
      "Current Row= 1190000\n",
      "Current Row= 1200000\n",
      "Current Row= 1210000\n",
      "Current Row= 1220000\n",
      "Current Row= 1230000\n",
      "Current Row= 1240000\n",
      "Current Row= 1250000\n",
      "Current Row= 1260000\n",
      "Current Row= 1270000\n",
      "Current Row= 1280000\n",
      "Current Row= 1290000\n",
      "Current Row= 1300000\n",
      "Current Row= 1310000\n",
      "Current Row= 1320000\n",
      "Current Row= 1330000\n",
      "Current Row= 1340000\n",
      "Current Row= 1350000\n",
      "Current Row= 1360000\n",
      "Current Row= 1370000\n",
      "Current Row= 1380000\n",
      "Current Row= 1390000\n",
      "Current Row= 1400000\n",
      "Current Row= 1410000\n",
      "Current Row= 1420000\n",
      "Current Row= 1430000\n",
      "Current Row= 1440000\n",
      "Current Row= 1450000\n",
      "Current Row= 1460000\n",
      "Current Row= 1470000\n",
      "Current Row= 1480000\n",
      "Current Row= 1490000\n",
      "Current Row= 1500000\n",
      "Current Row= 1510000\n",
      "Current Row= 1520000\n",
      "Current Row= 1530000\n",
      "Current Row= 1540000\n",
      "Current Row= 1550000\n",
      "Current Row= 1560000\n",
      "Current Row= 1570000\n",
      "Current Row= 1580000\n",
      "Current Row= 1590000\n",
      "Current Row= 1600000\n",
      "Current Row= 1610000\n",
      "Current Row= 1620000\n",
      "Current Row= 1630000\n",
      "Current Row= 1640000\n",
      "Current Row= 1650000\n",
      "Current Row= 1660000\n",
      "Current Row= 1670000\n",
      "Current Row= 1680000\n",
      "Current Row= 1690000\n",
      "Current Row= 1700000\n",
      "Current Row= 1710000\n",
      "Current Row= 1720000\n",
      "Current Row= 1730000\n",
      "Current Row= 1740000\n",
      "Current Row= 1750000\n",
      "Current Row= 1760000\n",
      "Current Row= 1770000\n",
      "Current Row= 1780000\n",
      "Current Row= 1790000\n",
      "Current Row= 1800000\n",
      "Current Row= 1810000\n",
      "Current Row= 1820000\n",
      "Current Row= 1830000\n",
      "Current Row= 1840000\n",
      "Current Row= 1850000\n",
      "Current Row= 1860000\n",
      "Current Row= 1870000\n",
      "Current Row= 1880000\n",
      "Current Row= 1890000\n",
      "Current Row= 1900000\n",
      "Current Row= 1910000\n",
      "Current Row= 1920000\n",
      "Current Row= 1930000\n",
      "Current Row= 1940000\n",
      "Current Row= 1950000\n",
      "Current Row= 1960000\n",
      "Current Row= 1970000\n",
      "Current Row= 1980000\n",
      "Current Row= 1990000\n",
      "Current Row= 2000000\n",
      "Current Row= 2010000\n",
      "Current Row= 2020000\n",
      "Current Row= 2030000\n",
      "Current Row= 2040000\n",
      "Current Row= 2050000\n",
      "Current Row= 2060000\n",
      "Current Row= 2070000\n",
      "Current Row= 2080000\n",
      "Current Row= 2090000\n",
      "Current Row= 2100000\n",
      "Current Row= 2110000\n",
      "Current Row= 2120000\n",
      "Current Row= 2130000\n",
      "Current Row= 2140000\n",
      "Current Row= 2150000\n",
      "Current Row= 2160000\n",
      "Current Row= 2170000\n",
      "Current Row= 2180000\n",
      "Current Row= 2190000\n",
      "Current Row= 2200000\n",
      "Current Row= 2210000\n",
      "Current Row= 2220000\n",
      "Current Row= 2230000\n",
      "Current Row= 2240000\n",
      "Current Row= 2250000\n",
      "Current Row= 2260000\n",
      "Current Row= 2270000\n",
      "Current Row= 2280000\n",
      "Current Row= 2290000\n",
      "Current Row= 2300000\n",
      "Current Row= 2310000\n",
      "Current Row= 2320000\n",
      "Current Row= 2330000\n",
      "Current Row= 2340000\n"
     ]
    }
   ],
   "source": [
    "# Feature creation\n",
    "dftest = pd.DataFrame.from_csv(\"./data/test.csv\")\n",
    "testfeaturesdf = FeatureEngineering(dftest, \"Mtest.csv\")\n",
    "dftest ={}\n",
    "testfeaturesdf={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
